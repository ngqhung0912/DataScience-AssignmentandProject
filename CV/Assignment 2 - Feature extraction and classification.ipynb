{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609bcc26",
   "metadata": {},
   "source": [
    "# Assignment 2 - Feature extraction and classification\n",
    "\n",
    "Note: This notebook file for the assignment has deviations from the course guide with respect to the structure, sentence framing, question framing and numbering. Please consider this notebook file structure as the final structure and follow this.\n",
    "\n",
    "In this assignment, you are expected to\n",
    "\n",
    "(1) extract global features from CIFAR10 dataset with one of the pre-trained neural networks available in pytorch,\n",
    "\n",
    "(2) classify the dataset using the traditional k-Nearest Neighbours classifier,\n",
    "\n",
    "and\n",
    "\n",
    "(3) implement k-fold cross-validation to evaluate your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b394f7",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4d6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the needed packages for this assignment here\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838957a",
   "metadata": {},
   "source": [
    "When working with Pytorch, dataloader() is a must to know function. Read more about this function and the parameters it accepts in https://blog.paperspace.com/dataloaders-abstractions-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb7db27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2320b0",
   "metadata": {},
   "source": [
    "The variable 'transform' encapsulates the needed transformations of our data. Read more about transforms in https://blog.paperspace.com/dataloaders-abstractions-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3daee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # resize\n",
    "    transforms.Resize(32),\n",
    "    # center-crop\n",
    "    transforms.CenterCrop(32),\n",
    "    # to-tensor\n",
    "    transforms.ToTensor(),\n",
    "    # normalize\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254f0e38",
   "metadata": {},
   "source": [
    "### INPUT DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ea732",
   "metadata": {},
   "source": [
    "Load the CIFAR10 dataset from Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53bb75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.CIFAR10(root='cifar10', train=True, download=False, transform=transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "dataset_test = torchvision.datasets.CIFAR10(root='cifar10', train=False, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb189026",
   "metadata": {},
   "source": [
    "#### Exercise 2.1 - Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57aaaa8",
   "metadata": {},
   "source": [
    "**a)** Write a function **'train_test_split(dataset, ratio)'** which takes a dataset array as an input and returns two dataset arrays- one for training and another for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b5f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f160f",
   "metadata": {},
   "source": [
    "### FEATURE EXTRACTION\n",
    "\n",
    "Extract descriptors from the images in your train and test dataset. The dataset split should remain the same for all the experiments if you want to be fair when comparing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032fccdb",
   "metadata": {},
   "source": [
    "#### Exercise 2.2 - Feature 1 - RGB descriptor\n",
    "\n",
    "Implement the same code you wrote for extracting the overall RGB descriptors(of size n x 24) as in assignment 1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f654967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.2.2 your code here\n",
    "\n",
    "def extract_rgb_descriptors(datasets):\n",
    "    dataset_list = []\n",
    "    for image, label in datasets:\n",
    "        image_feature = []\n",
    "        # image_feature_with_label = []\n",
    "        colors = (\"red\", \"green\", \"blue\")\n",
    "        image = np.array(image)\n",
    "        # print(image.shape)\n",
    "        # plt.imshow(image)\n",
    "        # break\n",
    "        # image_feature_with_label.append(label)\n",
    "        for channel_id, color in enumerate(colors):\n",
    "            histogram, bin_edges = np.histogram(\n",
    "                image[:, :, channel_id], bins=8, range=(0, 256)\n",
    "            )\n",
    "            image_feature = np.concatenate((image_feature, histogram))\n",
    "        # print(image_feature)\n",
    "        image_feature = image_feature / 784\n",
    "        image_feature = np.append(image_feature,label)\n",
    "        dataset_list.append(image_feature)\n",
    "    # training_data, val_data, train_label, val_label = train_test_split(train_df, label_list, test_size=0.2, random_state=42)\n",
    "    return pd.DataFrame(dataset_list)\n",
    "\n",
    "rgb_descriptors_df = extract_rgb_descriptors(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51372d11",
   "metadata": {},
   "source": [
    "#### Exercise 2.3 - Feature 2 - Extract CNN descriptors using pre-traind networks\n",
    "\n",
    "Load one of the pretrained network (resnet, alexnet, vgg, squeezenet, densenet, inception) from pytorch to extract global features from the images present in the dataset. \n",
    "We will use the output values from the layer present just before the fully connected layer of the deep network as a descriptor, i.e. we will remove the last fully-connected layer. Therefore, after feed-forwarding the input image through the network, we save the output as the descriptor of the image. We do this for all the images present in the dataset to get the overall CNN descriptors.\n",
    "\n",
    "You may refer to this link for debugging purposes - https://stackoverflow.com/questions/52548174/how-to-remove-the-last-fc-layer-from-a-resnet-model-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e04373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      ")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected 4D input (got 3D input)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m model_without_fc \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mSequential(\u001B[38;5;241m*\u001B[39m(\u001B[38;5;28mlist\u001B[39m(model\u001B[38;5;241m.\u001B[39mchildren())[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]))\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(model_without_fc)\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mmodel_without_fc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/miniforge3/envs/DS-CVassignment/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniforge3/envs/DS-CVassignment/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/miniforge3/envs/DS-CVassignment/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniforge3/envs/DS-CVassignment/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:135\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 135\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_input_dim\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;66;03m# exponential_average_factor is set to self.momentum\u001B[39;00m\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;66;03m# (when it is available) only so that it gets updated\u001B[39;00m\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;66;03m# in ONNX graph when this node is exported to ONNX.\u001B[39;00m\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmomentum \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniforge3/envs/DS-CVassignment/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:407\u001B[0m, in \u001B[0;36mBatchNorm2d._check_input_dim\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_input_dim\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    406\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m4\u001B[39m:\n\u001B[0;32m--> 407\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpected 4D input (got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124mD input)\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim()))\n",
      "\u001B[0;31mValueError\u001B[0m: expected 4D input (got 3D input)"
     ]
    }
   ],
   "source": [
    "from torchvision.models import ResNet18_Weights, resnet18\n",
    "\n",
    "# Ex.2.3 your code here\n",
    "# name of the model you wish to use - it should be selected from this list\n",
    "# [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "7599bb2b",
   "metadata": {},
   "source": [
    "### PERFORMANCE EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a59fdf",
   "metadata": {},
   "source": [
    "#### Exercise 2.4 - Error function\n",
    "\n",
    "Implement a function to evaluate the accuracy of your prediction. We will rely on the evaluation metric 'accuracy'.\n",
    "\n",
    "You are suggested to also use f-score, recall and precision. Have a look at https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3677bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(actual, predicted):\n",
    "    \n",
    "    # Ex.2.4 your code here\n",
    "    \n",
    "    return accuracy_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b447784",
   "metadata": {},
   "source": [
    "### TRAIN AND TEST YOUR MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab251607",
   "metadata": {},
   "source": [
    "#### Exercise 2.5 - k Nearest Neighbour model\n",
    "\n",
    "For this exercise, first split the extracted overall RGB and CNN descriptor to train and test sets with the help of the 'train_test_split()' function that you implemented before.\n",
    "\n",
    "**a)** Apply the classifier with different values of k (number of nearest neighbours) to the train **RGB descriptor** set and evaluate the performance of your models using the accuracy_metric() function that you implemented before.\n",
    "\n",
    "You can have a look at the documentation to understand the parameters that define the learning of the model,\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1470832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Use your k-NN - play with the value of the parameters to see how the model performs\n",
    "kvalue_list = [2,4,6,10,15] \n",
    "\n",
    "# Ex.2.5a your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f26dee-1c7b-4498-a6aa-320a2106a5b9",
   "metadata": {},
   "source": [
    "**b)** Apply the classifier with different values of k (number of nearest neighbours) to the train **CNN descriptor** and evaluate the performance of your models using the accuracy_metric() function that you implemented before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90aba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.2.5b your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e94c23",
   "metadata": {},
   "source": [
    "#### Exercise 2.6 - Visualize results \n",
    "\n",
    "**a)** Since you already applied PCA to the extracted overall RGB descriptor in assignment 1, now apply PCA to the extracted overall **CNN descriptor**.\n",
    "\n",
    "Steps to follow:\n",
    "\n",
    "1) Choose the kNN classifier with k value that gave you the best results in the previous exercise and use it to make predictions on your train CNN descriptor set.\n",
    "\n",
    "2) Apply PCA on the train set and select the first 2 principal components to represent each sample.\n",
    "\n",
    "2) Plot the principal components representing the samples with empty circles. Use one color per ground truth class lables. On top of this, plot the samples again but now with filled circles. For these filled circles, use the color of the class predicted per sample in step 1. You can note that misclassifications will make the colours not coincide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c1cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.2.6a your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a156163-1ee6-42f9-a187-d0738caebf81",
   "metadata": {},
   "source": [
    "**b)** Repeat the steps mentioned before but now on the test CNN descriptor set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86aaad-02cb-4919-826f-0bb254a29da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.2.6b your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77cbfb5",
   "metadata": {},
   "source": [
    "#### Exercise 2.7 - kNN with k-Fold cross-validation\n",
    "\n",
    "Assess the performance of your implemented kNN using k-Fold cross-validation. \n",
    "\n",
    "Run your implemented function evaluating for k (fold) = 2, 5 and 10. You can rely on the kNN that performed best in the previous exercises.\n",
    "Report the average accuracy and the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f603d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Ex.2.7 your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba435dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUGGESTION ON HOW TO PRESENT PERFORMANCE OF YOUR KFOLD CROSS VALIDATION ANALYSIS\n",
    "\n",
    "print('Summary results:')\n",
    "print(' ')\n",
    "print(' ')\n",
    "for i,k in enumerate(k_list):\n",
    "    print(k,'-fold cross validation:')  \n",
    "    print('Accuracies per fold: ', avg_acc_list[i]) \n",
    "    \n",
    "    avg_acc = round(sum(avg_acc_list[i])/k,2)\n",
    "    std_list= round(np.std(avg_acc_list[i]),2)\n",
    "    print('Average accuracy: ', avg_acc,'+-', std_list) \n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf70eb",
   "metadata": {},
   "source": [
    "### [Optional] Exercise: further explore by: \n",
    "- implement other classifiers such as SVM or Random Forest, \n",
    "- extract other descriptors from the images such as objects or other local features,\n",
    "- implement the evaluation metrics: recall, precision and f-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a83da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
